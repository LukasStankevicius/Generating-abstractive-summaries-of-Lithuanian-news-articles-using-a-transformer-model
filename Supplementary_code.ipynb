{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmGkKZkSZMxKnlL7+Bu2vF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukasStankevicius/Generating-abstractive-summaries-of-Lithuanian-news-articles-using-a-transformer-model/blob/main/Supplementary_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_axvbv0a38n8"
      },
      "source": [
        "This is a supplementary code material for our work [\"Generating abstractive summaries of Lithuanian news articles using a transformer model\"](https://arxiv.org/abs/2105.03279)  \n",
        "If you use this code for your research, please cite\n",
        "```bibtex\n",
        "@misc{stankevičius2021generating,\n",
        "      title={Generating abstractive summaries of Lithuanian news articles using a transformer model}, \n",
        "      author={Lukas Stankevičius and Mantas Lukoševičius},\n",
        "      year={2021},\n",
        "      eprint={2105.03279},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "```\n",
        "# Contents:\n",
        "* [Simple usage](#simple_usage)\n",
        "* [Advanced usage](#advances_usage)\n",
        "* [Automatic evaluation](#evaluation)\n",
        "* [How we trained the tokenizer](#tokenizer)\n",
        "* [How we trained the model](#training_model)\n",
        " * [Optimizer and scheduler](#opt)\n",
        " * [Data](#data)\n",
        " * [Final training script](#final)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDz3MLT-vWSx"
      },
      "source": [
        "Install libraries that we will need in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYXOIK4Uib1F"
      },
      "source": [
        "! pip install transformers==4.3 sentencepiece protobuf rouge-score PyStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xSL2qnQCwng"
      },
      "source": [
        "Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vVhR3avCLF0"
      },
      "source": [
        "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Adafactor, Trainer\n",
        "from rouge_score import rouge_scorer\n",
        "import Stemmer\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEE4bYk_DTe"
      },
      "source": [
        "#Simple usage<a name='simple_usage'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbjusyDl_Fmy"
      },
      "source": [
        "name= \"LukasStankevicius/t5-base-lithuanian-news-summaries-175\"\n",
        "my_pipeline = pipeline(task=\"text2text-generation\", model=name, framework=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ttTh2qdFcQq"
      },
      "source": [
        "Given the following article body from [15min](https://www.15min.lt/24sek/naujiena/lietuva/tarp-penkiu-rezultatyviausiu-tsrs-rinktines-visu-laiku-zaideju-trys-lietuviai-875-1380030):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CXBzVNCFgSv"
      },
      "source": [
        "text = \"\"\"\n",
        "Latvijos krepšinio legenda Valdis Valteris pirmadienį socialiniame tinkle pasidalino statistika, kurios viršūnėje yra Arvydas Sabonis.\n",
        "1982 metais TSRS rinktinėje debiutavęs 222 cm ūgio vidurio puolėjas su raudona apranga sužaidė 52 rungtynes, per kurias rinko po 15,6 taško. Tai pats aukščiausias rezultatyvumo vidurkis tarp visų sovietų komandai atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė ne mažiau nei 50 rungtynių. Antras šioje rikiuotėje kitas buvęs Kauno „Žalgirio“ krepšininkas Rimas Kurtinaitis. Jis debiutavo TSRS rinktinėje vėliau nei Sabas, – 1984 metais, bet irgi sužaidė 52 mačus. R.Kurtinaitis pelnė po 15 taškų. 25-ių rezultatyviausių žaidėjų sąrašu pasidalinęs latvis V.Valteris, pelnęs po 13,8 taško, yra trečias.\n",
        "Ketvirtas yra iš Kazachstano kilęs Valerijus Tichonenka, pelnęs po 13,7 taško per 79 rungtynes. Rezultatyviausią visų laikų TSRS rinktinės penketą uždaro Modestas Paulauskas. Lietuvos krepšinio legenda pelnė po 13,6 taško per 84 mačus.\n",
        "Dešimtuke taip pat yra Oleksandras Volkovas (po 13,5 taško), Sergejus Belovas (12,7), Anatolijus Myškinas (po 12,3), Vladimiras Tkačenka (11,7) ir Aleksandras Salnikovas (11,4). Dvyliktas šiame sąraše yra Valdemaras Chomičius, vidutiniškai rinkęs po 10 taškų, o keturioliktas dar vienas buvęs žalgirietis Sergejus Jovaiša (po 9,8 taško). Šarūno Marčiulionio rezultatyvumo vidurkis turėjo būti aukštesnis, bet jis sužaidė mažiau nei 50 rungtynių. Kaip žinia, Lietuvai išsilaisvinus ir atkūrus Nepriklausomybę, visi minėti mūsų šalies krepšininkai, išskyrus karjerą jau baigusį M.Paulauską, užsivilko žalią aprangą ir atstovavo savo tėvynei.\n",
        "A.Sabonis pagal rezultatyvumo vidurkį yra pirmas – jis Lietuvos rinktinei pelnė po 20 taškų. Antras pagal taškų vidurkį yra Artūras Karnišovas, rinkęs po 18,2 taško ir pelnęs iš viso daugiausiai taškų atstovaujant Lietuvos rinktinei (1453).\n",
        "Tarp žaidėjų, kurie sužaidė bent po 50 oficialių rungtynių Lietuvos rinktinėje, trečią vietą užima Ramūnas Šiškauskas (po 12,9), ketvirtąją Linas Kleiza (po 12,7 taško), o penktas – Saulius Štombergas (po 11,1 taško). Daugiausiai rungtynių Lietuvos rinktinėje sužaidęs ir daugiausiai olimpinių medalių (3) su ja laimėjęs Gintaras Einikis rinko po 9,6 taško, o pirmajame trejete pagal rungtynių skaičių ir pelnytus taškus esantis Šarūnas Jasikevičius pelnė po 9,9 taško.\n",
        "\"\"\"\n",
        "text = ' '.join(text.strip().split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwmGq53jFjEM"
      },
      "source": [
        "The summary can be obtained by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uRkrkxy6Fl9E",
        "outputId": "3fadaa24-bbbc-4b6c-f389-f8706a2b1879"
      },
      "source": [
        "my_pipeline(text)[0][\"generated_text\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Lietuvos krepšinio federacijos (LKF) prezidento Arvydo Sabonio rezultatyvumo vidurkis yra aukščiausias tarp visų Sovietų Sąjungos rinktinėje atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė bent po 50 oficialių rungtynių.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cLgOtz555bx"
      },
      "source": [
        "#Advanced usage<a name='advances_usage'></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6CLSPw_5yLE"
      },
      "source": [
        "name= \"LukasStankevicius/t5-base-lithuanian-news-summaries-175\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(name)\n",
        "def decode(x):\n",
        "    return tokenizer.decode(x, skip_special_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rURAJ-uu6Ke4"
      },
      "source": [
        "Given the following article body from [15min](https://www.15min.lt/24sek/naujiena/lietuva/tarp-penkiu-rezultatyviausiu-tsrs-rinktines-visu-laiku-zaideju-trys-lietuviai-875-1380030):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHIEDIII6J5t"
      },
      "source": [
        "text = \"\"\"\n",
        "Latvijos krepšinio legenda Valdis Valteris pirmadienį socialiniame tinkle pasidalino statistika, kurios viršūnėje yra Arvydas Sabonis.\n",
        "1982 metais TSRS rinktinėje debiutavęs 222 cm ūgio vidurio puolėjas su raudona apranga sužaidė 52 rungtynes, per kurias rinko po 15,6 taško. Tai pats aukščiausias rezultatyvumo vidurkis tarp visų sovietų komandai atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė ne mažiau nei 50 rungtynių. Antras šioje rikiuotėje kitas buvęs Kauno „Žalgirio“ krepšininkas Rimas Kurtinaitis. Jis debiutavo TSRS rinktinėje vėliau nei Sabas, – 1984 metais, bet irgi sužaidė 52 mačus. R.Kurtinaitis pelnė po 15 taškų. 25-ių rezultatyviausių žaidėjų sąrašu pasidalinęs latvis V.Valteris, pelnęs po 13,8 taško, yra trečias.\n",
        "Ketvirtas yra iš Kazachstano kilęs Valerijus Tichonenka, pelnęs po 13,7 taško per 79 rungtynes. Rezultatyviausią visų laikų TSRS rinktinės penketą uždaro Modestas Paulauskas. Lietuvos krepšinio legenda pelnė po 13,6 taško per 84 mačus.\n",
        "Dešimtuke taip pat yra Oleksandras Volkovas (po 13,5 taško), Sergejus Belovas (12,7), Anatolijus Myškinas (po 12,3), Vladimiras Tkačenka (11,7) ir Aleksandras Salnikovas (11,4). Dvyliktas šiame sąraše yra Valdemaras Chomičius, vidutiniškai rinkęs po 10 taškų, o keturioliktas dar vienas buvęs žalgirietis Sergejus Jovaiša (po 9,8 taško). Šarūno Marčiulionio rezultatyvumo vidurkis turėjo būti aukštesnis, bet jis sužaidė mažiau nei 50 rungtynių. Kaip žinia, Lietuvai išsilaisvinus ir atkūrus Nepriklausomybę, visi minėti mūsų šalies krepšininkai, išskyrus karjerą jau baigusį M.Paulauską, užsivilko žalią aprangą ir atstovavo savo tėvynei.\n",
        "A.Sabonis pagal rezultatyvumo vidurkį yra pirmas – jis Lietuvos rinktinei pelnė po 20 taškų. Antras pagal taškų vidurkį yra Artūras Karnišovas, rinkęs po 18,2 taško ir pelnęs iš viso daugiausiai taškų atstovaujant Lietuvos rinktinei (1453).\n",
        "Tarp žaidėjų, kurie sužaidė bent po 50 oficialių rungtynių Lietuvos rinktinėje, trečią vietą užima Ramūnas Šiškauskas (po 12,9), ketvirtąją Linas Kleiza (po 12,7 taško), o penktas – Saulius Štombergas (po 11,1 taško). Daugiausiai rungtynių Lietuvos rinktinėje sužaidęs ir daugiausiai olimpinių medalių (3) su ja laimėjęs Gintaras Einikis rinko po 9,6 taško, o pirmajame trejete pagal rungtynių skaičių ir pelnytus taškus esantis Šarūnas Jasikevičius pelnė po 9,9 taško.\n",
        "\"\"\"\n",
        "text = ' '.join(text.strip().split())\n",
        "input_dict = tokenizer(text,  padding=True, return_tensors=\"pt\", return_attention_mask=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrovUqYv6PIU"
      },
      "source": [
        "And generation parameters ([documentation](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate), [explanation](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwonPT656YKu"
      },
      "source": [
        "g_kwargs = dict(max_length=512, num_beams=10, no_repeat_ngram_size=2, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0N2WovM6dUE"
      },
      "source": [
        "The summary can be obtained by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jexv864o6eZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c6edd3df-7ec5-4d5c-ea42-d3f07ce8b6ac"
      },
      "source": [
        "output = model.generate(**input_dict, **g_kwargs)\n",
        "list(map(decode, output.tolist()))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Lietuvos krepšinio federacijos (LKF) prezidento Arvydo Sabonio rezultatyvumo vidurkis yra aukščiausias tarp visų Sovietų Sąjungos rinktinėje atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė bent po 50 oficialių rungtynių.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q6z9iEZ6kXZ"
      },
      "source": [
        "If you do a lot of compute you can take advantage of GPU (of course if you have one). Obtain summary with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkLR4y3-6l4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "099aa81e-2c43-454c-9265-31a3d4cb63f3"
      },
      "source": [
        "input_dict = {key:value.to(\"cuda:0\") for key, value in input_dict.items()}\n",
        "model = model.to(\"cuda:0\")\n",
        "output = model.generate(**input_dict, **g_kwargs)\n",
        "list(map(decode, output.cpu().tolist()))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Lietuvos krepšinio federacijos (LKF) prezidento Arvydo Sabonio rezultatyvumo vidurkis yra aukščiausias tarp visų Sovietų Sąjungos rinktinėje atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė bent po 50 oficialių rungtynių.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTYQCWQq6kMB"
      },
      "source": [
        "# Automatic evaluation<a name='evaluation'></a>\n",
        "We evaluated summaries with [ROUGE](https://www.aclweb.org/anthology/W04-1013/). It measures *n-gram* overlap between reference and generated texts. However, one should not completely trust it as the same meaning can be expressed by different words (*n-grams*). Yet it is almost the best we can do (automated and fast). Lithuanian language is quite rich with different word stem endings so we also \"helped\" ROUGE by stemming words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tEJ0-sL62sd"
      },
      "source": [
        "Combining the two:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_syCdWs63_8"
      },
      "source": [
        "class MyStemmer:\n",
        "    def __init__(self):\n",
        "        self.stemmer = Stemmer.Stemmer('lithuanian')\n",
        "\n",
        "    def stem(self, token):\n",
        "        return self.stemmer.stemWord(token)\n",
        "\n",
        "\n",
        "class MyRougeScorer(rouge_scorer.RougeScorer):\n",
        "    # I rewrite init to have different stemmer\n",
        "    def __init__(self, rouge_types, use_stemmer=False):\n",
        "        self.rouge_types = rouge_types\n",
        "        self._stemmer = MyStemmer() if use_stemmer else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MixnNJ-6-vN"
      },
      "source": [
        "Now, given the gold reference and generated summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjBl8zJv7AAm"
      },
      "source": [
        "ground_truth = \"Kai Lietuva dar buvo okupuota ir mūsų šalies krepšininkai privalėjo žaisti TSRS rinktinėje, keli jų buvo ryškūs lyderiai.\"\n",
        "generated_text = \"Lietuvos krepšinio federacijos (LKF) prezidento Arvydo Sabonio rezultatyvumo vidurkis yra aukščiausias tarp visų Sovietų Sąjungos rinktinėje atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė bent po 50 oficialių rungtynių.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpglmZ9j7CqH"
      },
      "source": [
        "Let's calculate ROUGE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7axgM2F77FC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca40a73-e069-47f3-c65c-ec5a69a6af4f"
      },
      "source": [
        "rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
        "scorer = MyRougeScorer(rouge_types, use_stemmer=True)\n",
        "score = scorer.score(ground_truth, generated_text)\n",
        "print({s:score[s].fmeasure for s in rouge_types})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge1': 0.20689655172413793, 'rouge2': 0.03571428571428572, 'rougeL': 0.1724137931034483}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-hC7Un_7L-x"
      },
      "source": [
        "We monitored training by calculating ROUGE for 4096 validation pairs and noticed that after 250000 training steps our model started to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmQyEahV7Nln"
      },
      "source": [
        "# How we trained the tokenizer<a name='tokenizer'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af_jJLL3JAKr"
      },
      "source": [
        "Now we need a very big text file. Suppose we have one with over 1000000 lines in it and name it `\"my_big_text_file.txt\"`. Be warned that the following code requires a lot of memory (you can reduce number of lines sampled by lowering `input_sentence_size`) and can take several hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ZZbY1gH8zu"
      },
      "source": [
        "default_kwargs = {\n",
        "    \"model_type\": 'unigram', \"pad_id\": 0, \"eos_id\": 1, \"unk_id\": 2, \"bos_id\": -1, \"pad_piece\": '<pad>',\n",
        "    \"eos_piece\": '</s>',\n",
        "    \"unk_piece\": '<unk>', \"input_sentence_size\": 1000000, \"max_sentencepiece_length\": 64, \"add_dummy_prefix\": True\n",
        "}\n",
        "# more options are here: https://github.com/google/sentencepiece/blob/master/doc/options.md\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"my_big_text_file.txt\",\n",
        "    model_prefix=\"my_new_tokenizer\",\n",
        "    vocab_size=32000,\n",
        "    split_by_whitespace=True,\n",
        "    **default_kwargs\n",
        ")\n",
        "# normalization_rule_name=nmt_nfkc_cf if you want to lowercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGUj_8WAK1lp"
      },
      "source": [
        "Now that our sentencepiece model is trained, let's put it in our `T5Tokenizer` from `transformers` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc3F2zVZLDyy"
      },
      "source": [
        "tokenizer = T5Tokenizer(\"my_new_tokenizer.model\", do_lower_case=False)\n",
        "tokenizer._add_tokens(new_tokens=[f\"<extra_id_{i}>\" for i in range(100)] + ['</s>', '<pad>', '<unk>'],\n",
        "                      special_tokens=True)\n",
        "tokenizer.save_pretrained(\"MyNewT5Tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI2zjOF1Lqgk"
      },
      "source": [
        "So now you can load your trained tokenizer with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp8ftGyNLxD7"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"MyNewT5Tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wBGxZz-L5_6"
      },
      "source": [
        "# How we trained the model<a name='training_model'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjfXZHEoeEU2"
      },
      "source": [
        "## Optimizer and scheduler<a name='opt'></a>\n",
        "We used [T5](https://arxiv.org/abs/1910.10683) transformer model. It was originally trained using [Adafactor](https://arxiv.org/abs/1804.04235) optimizer. We used it with with 10 000 warm-up steps followed by inverse square root internal learning rate schedule. All of this is set internally, so we create `Dummy`, the fake learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZaDiH4MR1I0"
      },
      "source": [
        "class Dummy:\n",
        "    def step(self):\n",
        "        return 1\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return [1]\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\"dummy_key\": 1}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        pass\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj54dGjLg0ij"
      },
      "source": [
        "## Data<a name='data'></a>\n",
        "Our training corpus consisted of over 6GB text file and was to big to load into the Colab RAM. So we:  \n",
        "1. encoded it with our trained tokenizer - each string was converted to list of numbers from 0 to 32000;  \n",
        "2. as our maximum number is 32000, we changed type of our lists to numpy arrays of type `uint16` which can contain integers from 0 to 65535;  \n",
        "These \"tricks\" enabled us to load our pandas dataframe into Colab memory without memory errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Fj0gh9xlzM"
      },
      "source": [
        "For example purposes we will construct an example dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTQopwehxu2U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "b8af6f87-292b-4bfe-fbef-f31f3db41287"
      },
      "source": [
        "# this will produce 10 rows with exactly the same line\n",
        "df = pd.DataFrame.from_records(data=[(\"Čia yra naujienų straipsnio pagrindinė dalis.\",\"O čia yra santrauka.\")]*10, columns=[\"main\", \"summary\"])\n",
        "# load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"LukasStankevicius/t5-base-lithuanian-news-summaries-175\")\n",
        "# encode and reduce memory footprint with uint16 dtype\n",
        "for col in [\"main\", \"summary\"]:\n",
        "  df[col] = df[col].apply(tokenizer.encode, max_length=512, truncation=True)\n",
        "  df[col] = df[col].apply(np.asarray, dtype=np.uint16)\n",
        "# this will produce 400 000 rows with exactly the same line\n",
        "df = pd.concat([df for i in range(40000)])\n",
        "# shuffle rows\n",
        "df = df.sample(frac=1)\n",
        "# split to train and valid parts\n",
        "df.iloc[:-4096].to_pickle(\"my_pandas_train_dataframe_pickle.gz\")\n",
        "df.iloc[-4096:].to_pickle(\"my_pandas_valid_dataframe_pickle.gz\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>main</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[902, 22, 835, 1881, 3502, 401, 4, 1]</td>\n",
              "      <td>[133, 211, 22, 1992, 892, 26, 4, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[902, 22, 835, 1881, 3502, 401, 4, 1]</td>\n",
              "      <td>[133, 211, 22, 1992, 892, 26, 4, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[902, 22, 835, 1881, 3502, 401, 4, 1]</td>\n",
              "      <td>[133, 211, 22, 1992, 892, 26, 4, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[902, 22, 835, 1881, 3502, 401, 4, 1]</td>\n",
              "      <td>[133, 211, 22, 1992, 892, 26, 4, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[902, 22, 835, 1881, 3502, 401, 4, 1]</td>\n",
              "      <td>[133, 211, 22, 1992, 892, 26, 4, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    main                              summary\n",
              "8  [902, 22, 835, 1881, 3502, 401, 4, 1]  [133, 211, 22, 1992, 892, 26, 4, 1]\n",
              "1  [902, 22, 835, 1881, 3502, 401, 4, 1]  [133, 211, 22, 1992, 892, 26, 4, 1]\n",
              "5  [902, 22, 835, 1881, 3502, 401, 4, 1]  [133, 211, 22, 1992, 892, 26, 4, 1]\n",
              "6  [902, 22, 835, 1881, 3502, 401, 4, 1]  [133, 211, 22, 1992, 892, 26, 4, 1]\n",
              "5  [902, 22, 835, 1881, 3502, 401, 4, 1]  [133, 211, 22, 1992, 892, 26, 4, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REx8EJQ701Q2"
      },
      "source": [
        "The following are dataset (loading pairs) and colloator (combining individual pairs into batches) classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T27Jyhti7YM"
      },
      "source": [
        "class My_Dataset(Dataset):\n",
        "    def __init__(self, pickle_path):\n",
        "        df = pd.read_pickle(pickle_path)\n",
        "        self.examples = list(zip(df[\"main\"], df[\"summary\"]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "class MyCollator:\n",
        "    \"\"\"\n",
        "This collator is used for already encoded strings. It only truncates and pads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, list_of_tuples):\n",
        "        train_x, train_y = zip(*list_of_tuples)\n",
        "        # truncate\n",
        "        train_x, train_y = [seq[: self.max_length] for seq in train_x], [seq[: self.max_length] for seq in train_y]\n",
        "\n",
        "        # first the targets\n",
        "        n_items = len(train_y)\n",
        "        tt = self.tokenizer.pad({\"input_ids\": train_y}, padding=True,\n",
        "                                return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "        decoder_input_ids = torch.cat((torch.zeros(size=(n_items, 1), dtype=torch.int64), tt['input_ids']), axis=1)\n",
        "        decoder_attention_mask = torch.cat((torch.ones(size=(n_items, 1), dtype=torch.int64), tt['attention_mask']),\n",
        "                                           axis=1)\n",
        "\n",
        "        decoder_input_ids = decoder_input_ids[:, :-1]  # one item is added at beginning, so one at the end to remove\n",
        "        decoder_attention_mask = decoder_attention_mask[:, :-1]\n",
        "\n",
        "        # now inputs\n",
        "        inputs_dict = self.tokenizer.pad({\"input_ids\": train_x},  padding=True, return_tensors=\"pt\",\n",
        "                                         return_attention_mask=True)\n",
        "        # finally combine the two\n",
        "        return {\"decoder_input_ids\": decoder_input_ids, \"decoder_attention_mask\": decoder_attention_mask,\n",
        "                \"labels\": tt['input_ids'], **inputs_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aXLyCyhndVR"
      },
      "source": [
        "## Final training script<a name='final'></a>\n",
        "You will definitely need GPU here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_m5i0p2hCL6"
      },
      "source": [
        "output_dir = \"output_directory_for_my_model\"\n",
        "\n",
        "kwargs = TrainingArguments(\n",
        "    fp16=True, per_device_train_batch_size=4, gradient_accumulation_steps=32,\n",
        "    num_train_epochs=30, output_dir=output_dir, evaluation_strategy=\"steps\", \n",
        "    per_device_eval_batch_size=4, max_grad_norm=None, logging_steps=2000, \n",
        "    save_steps=5000, eval_steps=2000, dataloader_num_workers=1, adafactor=True\n",
        ")\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"LukasStankevicius/t5-base-lithuanian-news-summaries-175\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    train_dataset=My_Dataset(\"my_pandas_train_dataframe_pickle.gz\"), \n",
        "    eval_dataset=My_Dataset(\"my_pandas_valid_dataframe_pickle.gz\"),\n",
        "    model=model, data_collator=MyCollator(tokenizer), tokenizer=tokenizer,\n",
        "    args=kwargs, \n",
        "    optimizers=(Adafactor((param for param in model.parameters() if param.requires_grad),\n",
        "                           relative_step=True, warmup_init=True), Dummy()))\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "trainer.state.save_to_json(output_dir + \"/trainer_state.json\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}