{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNUS5I4yxOZ9DrVJ8pc4oL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukasStankevicius/Generating-abstractive-summaries-of-Lithuanian-news-articles-using-a-transformer-model/blob/main/Supplementary_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEE4bYk_DTe"
      },
      "source": [
        "#Simple usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYXOIK4Uib1F"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbjusyDl_Fmy"
      },
      "source": [
        "from transformers import pipeline\n",
        "name= \"LukasStankevicius/t5-base-lithuanian-news-summaries-175\"\n",
        "my_pipeline = pipeline(task=\"text2text-generation\", model=name, framework=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ttTh2qdFcQq"
      },
      "source": [
        "Given the following article body from [15min](https://www.15min.lt/24sek/naujiena/lietuva/tarp-penkiu-rezultatyviausiu-tsrs-rinktines-visu-laiku-zaideju-trys-lietuviai-875-1380030):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CXBzVNCFgSv"
      },
      "source": [
        "text = \"\"\"\n",
        "Latvijos krepšinio legenda Valdis Valteris pirmadienį socialiniame tinkle pasidalino statistika, kurios viršūnėje yra Arvydas Sabonis.\n",
        "1982 metais TSRS rinktinėje debiutavęs 222 cm ūgio vidurio puolėjas su raudona apranga sužaidė 52 rungtynes, per kurias rinko po 15,6 taško. Tai pats aukščiausias rezultatyvumo vidurkis tarp visų sovietų komandai atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė ne mažiau nei 50 rungtynių. Antras šioje rikiuotėje kitas buvęs Kauno „Žalgirio“ krepšininkas Rimas Kurtinaitis. Jis debiutavo TSRS rinktinėje vėliau nei Sabas, – 1984 metais, bet irgi sužaidė 52 mačus. R.Kurtinaitis pelnė po 15 taškų. 25-ių rezultatyviausių žaidėjų sąrašu pasidalinęs latvis V.Valteris, pelnęs po 13,8 taško, yra trečias.\n",
        "Ketvirtas yra iš Kazachstano kilęs Valerijus Tichonenka, pelnęs po 13,7 taško per 79 rungtynes. Rezultatyviausią visų laikų TSRS rinktinės penketą uždaro Modestas Paulauskas. Lietuvos krepšinio legenda pelnė po 13,6 taško per 84 mačus.\n",
        "Dešimtuke taip pat yra Oleksandras Volkovas (po 13,5 taško), Sergejus Belovas (12,7), Anatolijus Myškinas (po 12,3), Vladimiras Tkačenka (11,7) ir Aleksandras Salnikovas (11,4). Dvyliktas šiame sąraše yra Valdemaras Chomičius, vidutiniškai rinkęs po 10 taškų, o keturioliktas dar vienas buvęs žalgirietis Sergejus Jovaiša (po 9,8 taško). Šarūno Marčiulionio rezultatyvumo vidurkis turėjo būti aukštesnis, bet jis sužaidė mažiau nei 50 rungtynių. Kaip žinia, Lietuvai išsilaisvinus ir atkūrus Nepriklausomybę, visi minėti mūsų šalies krepšininkai, išskyrus karjerą jau baigusį M.Paulauską, užsivilko žalią aprangą ir atstovavo savo tėvynei.\n",
        "A.Sabonis pagal rezultatyvumo vidurkį yra pirmas – jis Lietuvos rinktinei pelnė po 20 taškų. Antras pagal taškų vidurkį yra Artūras Karnišovas, rinkęs po 18,2 taško ir pelnęs iš viso daugiausiai taškų atstovaujant Lietuvos rinktinei (1453).\n",
        "Tarp žaidėjų, kurie sužaidė bent po 50 oficialių rungtynių Lietuvos rinktinėje, trečią vietą užima Ramūnas Šiškauskas (po 12,9), ketvirtąją Linas Kleiza (po 12,7 taško), o penktas – Saulius Štombergas (po 11,1 taško). Daugiausiai rungtynių Lietuvos rinktinėje sužaidęs ir daugiausiai olimpinių medalių (3) su ja laimėjęs Gintaras Einikis rinko po 9,6 taško, o pirmajame trejete pagal rungtynių skaičių ir pelnytus taškus esantis Šarūnas Jasikevičius pelnė po 9,9 taško.\n",
        "\"\"\"\n",
        "text = ' '.join(text.strip().split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwmGq53jFjEM"
      },
      "source": [
        "The summary can be obtained by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRkrkxy6Fl9E"
      },
      "source": [
        "my_pipeline(text)[0][\"generated_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cLgOtz555bx"
      },
      "source": [
        "#Advanced usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6CLSPw_5yLE"
      },
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "name= \"LukasStankevicius/t5-base-lithuanian-news-summaries-175\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(name)\n",
        "def decode(x):\n",
        "    return tokenizer.decode(x, skip_special_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rURAJ-uu6Ke4"
      },
      "source": [
        "Given the following article body from [15min](https://www.15min.lt/24sek/naujiena/lietuva/tarp-penkiu-rezultatyviausiu-tsrs-rinktines-visu-laiku-zaideju-trys-lietuviai-875-1380030):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHIEDIII6J5t"
      },
      "source": [
        "text = \"\"\"\n",
        "Latvijos krepšinio legenda Valdis Valteris pirmadienį socialiniame tinkle pasidalino statistika, kurios viršūnėje yra Arvydas Sabonis.\n",
        "1982 metais TSRS rinktinėje debiutavęs 222 cm ūgio vidurio puolėjas su raudona apranga sužaidė 52 rungtynes, per kurias rinko po 15,6 taško. Tai pats aukščiausias rezultatyvumo vidurkis tarp visų sovietų komandai atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė ne mažiau nei 50 rungtynių. Antras šioje rikiuotėje kitas buvęs Kauno „Žalgirio“ krepšininkas Rimas Kurtinaitis. Jis debiutavo TSRS rinktinėje vėliau nei Sabas, – 1984 metais, bet irgi sužaidė 52 mačus. R.Kurtinaitis pelnė po 15 taškų. 25-ių rezultatyviausių žaidėjų sąrašu pasidalinęs latvis V.Valteris, pelnęs po 13,8 taško, yra trečias.\n",
        "Ketvirtas yra iš Kazachstano kilęs Valerijus Tichonenka, pelnęs po 13,7 taško per 79 rungtynes. Rezultatyviausią visų laikų TSRS rinktinės penketą uždaro Modestas Paulauskas. Lietuvos krepšinio legenda pelnė po 13,6 taško per 84 mačus.\n",
        "Dešimtuke taip pat yra Oleksandras Volkovas (po 13,5 taško), Sergejus Belovas (12,7), Anatolijus Myškinas (po 12,3), Vladimiras Tkačenka (11,7) ir Aleksandras Salnikovas (11,4). Dvyliktas šiame sąraše yra Valdemaras Chomičius, vidutiniškai rinkęs po 10 taškų, o keturioliktas dar vienas buvęs žalgirietis Sergejus Jovaiša (po 9,8 taško). Šarūno Marčiulionio rezultatyvumo vidurkis turėjo būti aukštesnis, bet jis sužaidė mažiau nei 50 rungtynių. Kaip žinia, Lietuvai išsilaisvinus ir atkūrus Nepriklausomybę, visi minėti mūsų šalies krepšininkai, išskyrus karjerą jau baigusį M.Paulauską, užsivilko žalią aprangą ir atstovavo savo tėvynei.\n",
        "A.Sabonis pagal rezultatyvumo vidurkį yra pirmas – jis Lietuvos rinktinei pelnė po 20 taškų. Antras pagal taškų vidurkį yra Artūras Karnišovas, rinkęs po 18,2 taško ir pelnęs iš viso daugiausiai taškų atstovaujant Lietuvos rinktinei (1453).\n",
        "Tarp žaidėjų, kurie sužaidė bent po 50 oficialių rungtynių Lietuvos rinktinėje, trečią vietą užima Ramūnas Šiškauskas (po 12,9), ketvirtąją Linas Kleiza (po 12,7 taško), o penktas – Saulius Štombergas (po 11,1 taško). Daugiausiai rungtynių Lietuvos rinktinėje sužaidęs ir daugiausiai olimpinių medalių (3) su ja laimėjęs Gintaras Einikis rinko po 9,6 taško, o pirmajame trejete pagal rungtynių skaičių ir pelnytus taškus esantis Šarūnas Jasikevičius pelnė po 9,9 taško.\n",
        "\"\"\"\n",
        "text = ' '.join(text.strip().split())\n",
        "input_dict = tokenizer(text,  padding=True, return_tensors=\"pt\", return_attention_mask=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrovUqYv6PIU"
      },
      "source": [
        "And generation parameters ([documentation](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate), [explanation](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwonPT656YKu"
      },
      "source": [
        "g_kwargs = dict(max_length=512, num_beams=10, no_repeat_ngram_size=2, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0N2WovM6dUE"
      },
      "source": [
        "The summary can be obtained by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jexv864o6eZo"
      },
      "source": [
        "output = model.generate(**input_dict, **g_kwargs)\n",
        "list(map(decode, output.tolist()))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q6z9iEZ6kXZ"
      },
      "source": [
        "If you do a lot of compute you can take advantage of GPU. Obtain summary with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkLR4y3-6l4i"
      },
      "source": [
        "input_dict = {key:value.to(\"cuda:0\") for key, value in input_dict.items()}\n",
        "model = model.to(\"cuda:0\")\n",
        "output = model.generate(**input_dict, **g_kwargs)\n",
        "list(map(decode, output.cpu().tolist()))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTYQCWQq6kMB"
      },
      "source": [
        "# Automatic evaluation\n",
        "We evaluated summaries with [ROUGE](https://www.aclweb.org/anthology/W04-1013/). It measures *n-gram* overlap between reference and generated texts. However, one should not completely trust it as the same meaning can be expressed by different words (*n-grams*). Yet it is almost the best we can do (automated and fast). Lithuanian language is quite rich with different word stem endings so we also \"helped\" ROUGE by stemming words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ROO93m06sHW"
      },
      "source": [
        "! pip install rouge-score PyStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tEJ0-sL62sd"
      },
      "source": [
        "Combining the two:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_syCdWs63_8"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import Stemmer\n",
        "\n",
        "class MyStemmer:\n",
        "    def __init__(self):\n",
        "        self.stemmer = Stemmer.Stemmer('lithuanian')\n",
        "\n",
        "    def stem(self, token):\n",
        "        return self.stemmer.stemWord(token)\n",
        "\n",
        "\n",
        "class MyRougeScorer(rouge_scorer.RougeScorer):\n",
        "    # I rewrite init to have different stemmer\n",
        "    def __init__(self, rouge_types, use_stemmer=False):\n",
        "        self.rouge_types = rouge_types\n",
        "        self._stemmer = MyStemmer() if use_stemmer else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MixnNJ-6-vN"
      },
      "source": [
        "Now, given the gold reference and generated summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjBl8zJv7AAm"
      },
      "source": [
        "ground_truth = \"Kai Lietuva dar buvo okupuota ir mūsų šalies krepšininkai privalėjo žaisti TSRS rinktinėje, keli jų buvo ryškūs lyderiai.\"\n",
        "generated_text = \"Lietuvos krepšinio federacijos (LKF) prezidento Arvydo Sabonio rezultatyvumo vidurkis yra aukščiausias tarp visų Sovietų Sąjungos rinktinėje atstovavusių žaidėjų, skaičiuojant tuos, kurie sužaidė bent po 50 oficialių rungtynių.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpglmZ9j7CqH"
      },
      "source": [
        "Let's calculate ROUGE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7axgM2F77FC2"
      },
      "source": [
        "rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
        "scorer = MyRougeScorer(rouge_types, use_stemmer=True)\n",
        "score = scorer.score(ground_truth, generated_text)\n",
        "print({s:score[s].fmeasure for s in rouge_types})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-hC7Un_7L-x"
      },
      "source": [
        "We monitored training by calculating ROUGE for 4096 validation pairs and noticed that after 250000 training steps our model started to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmQyEahV7Nln"
      },
      "source": [
        "# How we trained the tokenizer\n",
        "First let's install required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3WbK44mHOTc"
      },
      "source": [
        "! pip install sentencepiece protobuf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af_jJLL3JAKr"
      },
      "source": [
        "Now we need a very big text file. Suppose we have one with over 1000000 lines in it and name it `\"my_big_text_file.txt\"`. Be warned that the following code requires a lot of memory (you can reduce number of lines sampled by lowering `input_sentence_size`) and can take several hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ZZbY1gH8zu"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "default_kwargs = {\n",
        "    \"model_type\": 'unigram', \"pad_id\": 0, \"eos_id\": 1, \"unk_id\": 2, \"bos_id\": -1, \"pad_piece\": '<pad>',\n",
        "    \"eos_piece\": '</s>',\n",
        "    \"unk_piece\": '<unk>', \"input_sentence_size\": 1000000, \"max_sentencepiece_length\": 64, \"add_dummy_prefix\": True\n",
        "}\n",
        "# more options are here: https://github.com/google/sentencepiece/blob/master/doc/options.md\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"my_big_text_file.txt\",\n",
        "    model_prefix=\"my_new_tokenizer\",\n",
        "    vocab_size=32000,\n",
        "    split_by_whitespace=True,\n",
        "    **default_kwargs\n",
        ")\n",
        "# normalization_rule_name=nmt_nfkc_cf if you want to lowercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGUj_8WAK1lp"
      },
      "source": [
        "Now that our sentencepiece model is trained, let's put it in our `T5Tokenizer` from `transformers` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc3F2zVZLDyy"
      },
      "source": [
        "from transformers import T5Tokenizer\n",
        "tokenizer = T5Tokenizer(\"my_new_tokenizer.model\", do_lower_case=False)\n",
        "tokenizer._add_tokens(new_tokens=[f\"<extra_id_{i}>\" for i in range(100)] + ['</s>', '<pad>', '<unk>'],\n",
        "                      special_tokens=True)\n",
        "tokenizer.save_pretrained(\"MyNewT5Tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI2zjOF1Lqgk"
      },
      "source": [
        "So now you can load your trained tokenizer with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp8ftGyNLxD7"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"MyNewT5Tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wBGxZz-L5_6"
      },
      "source": [
        "# How we trained model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjfXZHEoeEU2"
      },
      "source": [
        "## Optimizer and scheduler\n",
        "We used [T5](https://arxiv.org/abs/1910.10683) transformer model. It was originally trained using [Adafactor](https://arxiv.org/abs/1804.04235) optimizer. We used it with with 10 000 warm-up steps followed by inverse square root internal learning rate schedule. Yet we still wanted to observe internal learning rate so we added some tweaks to original `transformers` library code. `Dummy` is for fake learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZaDiH4MR1I0"
      },
      "source": [
        "from transformers import Adafactor, Trainer\n",
        "\n",
        "\n",
        "class MyAdafactor(Adafactor):\n",
        "\n",
        "    def __init__(self, args, **kwargs):\n",
        "        super(MyAdafactor, self).__init__(args, **kwargs)\n",
        "        self.last_learning_rates = []\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "        all_lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    grad = grad.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"Adafactor does not support sparse gradients.\")\n",
        "\n",
        "                state = self.state[p]\n",
        "                grad_shape = grad.shape\n",
        "\n",
        "                factored, use_first_moment = self._get_options(group, grad_shape)\n",
        "                # State Initialization\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "\n",
        "                    if use_first_moment:\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state[\"exp_avg\"] = torch.zeros_like(grad)\n",
        "                    if factored:\n",
        "                        state[\"exp_avg_sq_row\"] = torch.zeros(grad_shape[:-1]).to(grad)\n",
        "                        state[\"exp_avg_sq_col\"] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).to(grad)\n",
        "                    else:\n",
        "                        state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
        "\n",
        "                    state[\"RMS\"] = 0\n",
        "                else:\n",
        "                    if use_first_moment:\n",
        "                        state[\"exp_avg\"] = state[\"exp_avg\"].to(grad)\n",
        "                    if factored:\n",
        "                        state[\"exp_avg_sq_row\"] = state[\"exp_avg_sq_row\"].to(grad)\n",
        "                        state[\"exp_avg_sq_col\"] = state[\"exp_avg_sq_col\"].to(grad)\n",
        "                    else:\n",
        "                        state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].to(grad)\n",
        "\n",
        "                p_data_fp32 = p.data\n",
        "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    p_data_fp32 = p_data_fp32.float()\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                state[\"RMS\"] = self._rms(p_data_fp32)\n",
        "                lr = self._get_lr(group, state)\n",
        "\n",
        "                beta2t = 1.0 - math.pow(state[\"step\"], group[\"decay_rate\"])\n",
        "                update = (grad ** 2) + group[\"eps\"][0]\n",
        "                if factored:\n",
        "                    exp_avg_sq_row = state[\"exp_avg_sq_row\"]\n",
        "                    exp_avg_sq_col = state[\"exp_avg_sq_col\"]\n",
        "\n",
        "                    exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
        "                    exp_avg_sq_col.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-2))\n",
        "\n",
        "                    # Approximation of exponential moving average of square of gradient\n",
        "                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)\n",
        "                    update.mul_(grad)\n",
        "                else:\n",
        "                    exp_avg_sq = state[\"exp_avg_sq\"]\n",
        "\n",
        "                    exp_avg_sq.mul_(beta2t).add_(1.0 - beta2t, update)\n",
        "                    update = exp_avg_sq.rsqrt().mul_(grad)\n",
        "\n",
        "                update.div_((self._rms(update) / group[\"clip_threshold\"]).clamp_(min=1.0))\n",
        "                update.mul_(lr)\n",
        "\n",
        "                if use_first_moment:\n",
        "                    exp_avg = state[\"exp_avg\"]\n",
        "                    exp_avg.mul_(group[\"beta1\"]).add_(1 - group[\"beta1\"], update)\n",
        "                    update = exp_avg\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p_data_fp32.add_(-group[\"weight_decay\"] * lr, p_data_fp32)\n",
        "\n",
        "                p_data_fp32.add_(-update)\n",
        "\n",
        "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "                all_lr.append(lr)\n",
        "        self.last_learning_rates = torch.hstack(all_lr).cpu().numpy().tolist()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Dummy:\n",
        "    def step(self):\n",
        "        return 1\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return [1]\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\"dummy_key\": 1}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        pass\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [1]\n",
        "\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "      def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch):\n",
        "        if self.control.should_log:\n",
        "            logs: Dict[str, float] = {}\n",
        "            tr_loss_scalar = tr_loss.item()\n",
        "            # reset tr_loss to zero\n",
        "            tr_loss -= tr_loss\n",
        "\n",
        "            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n",
        "            # backward compatibility for pytorch schedulers\n",
        "            if self.args.adafactor:\n",
        "                logs[\"learning_rate\"] = self.optimizer.last_learning_rates\n",
        "            else:\n",
        "                logs[\"learning_rate\"] = (\n",
        "                    self.lr_scheduler.get_last_lr()[0]\n",
        "                    if version.parse(torch.__version__) >= version.parse(\"1.4\")\n",
        "                    else self.lr_scheduler.get_lr()[0]\n",
        "                )\n",
        "            self._total_loss_scalar += tr_loss_scalar\n",
        "            self._globalstep_last_logged = self.state.global_step\n",
        "\n",
        "            self.log(logs)\n",
        "\n",
        "        metrics = None\n",
        "        if self.control.should_evaluate:\n",
        "            metrics = self.evaluate()\n",
        "            self._report_to_hp_search(trial, epoch, metrics)\n",
        "\n",
        "        if self.control.should_save:\n",
        "            self._save_checkpoint(model, trial, metrics=metrics)\n",
        "            self.control = self.callback_handler.on_save(self.args, self.state, self.control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj54dGjLg0ij"
      },
      "source": [
        "## Data\n",
        "Our training corpus consisted of over 6GB text file and was to big to load into the Colab RAM. So we:  \n",
        "1) encoded it with our trained tokenizer - each string was converted to list of numbers from 0 to 32000;  \n",
        "2) as our maximum number is 32000, we changed type of our lists to numpy arrays of type `uint16` which can contain integers from 0 to 65535;  \n",
        "These \"tricks\" enabled us to load our pandas dataframe into Colab memory without memory errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T27Jyhti7YM"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class My_Dataset(Dataset):\n",
        "    def __init__(self, pickle_path):\n",
        "        df = pd.read_pickle(pickle_path)\n",
        "        self.examples = list(zip(df[\"main\"], df[\"summary\"]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "class MyCollator2:\n",
        "    \"\"\"\n",
        "This collator is used for already encoded strings. It only truncates and pads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, list_of_tuples):\n",
        "        train_x, train_y = zip(*list_of_tuples)\n",
        "        # truncate\n",
        "        train_x, train_y = [seq[: self.max_length] for seq in train_x], [seq[: self.max_length] for seq in train_y]\n",
        "\n",
        "        # first the targets\n",
        "        n_items = len(train_y)\n",
        "        tt = self.tokenizer.pad({\"input_ids\": train_y}, padding=True,\n",
        "                                return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "        decoder_input_ids = torch.cat((torch.zeros(size=(n_items, 1), dtype=torch.int64), tt['input_ids']), axis=1)\n",
        "        decoder_attention_mask = torch.cat((torch.ones(size=(n_items, 1), dtype=torch.int64), tt['attention_mask']),\n",
        "                                           axis=1)\n",
        "\n",
        "        decoder_input_ids = decoder_input_ids[:, :-1]  # one item is added at beginning, so one at the end to remove\n",
        "        decoder_attention_mask = decoder_attention_mask[:, :-1]\n",
        "\n",
        "        # now inputs\n",
        "        inputs_dict = self.tokenizer.pad({\"input_ids\": train_x},  padding=True, return_tensors=\"pt\",\n",
        "                                         return_attention_mask=True)\n",
        "        # finally combine the two\n",
        "        return {\"decoder_input_ids\": decoder_input_ids, \"decoder_attention_mask\": decoder_attention_mask,\n",
        "                \"labels\": tt['input_ids'], **inputs_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aXLyCyhndVR"
      },
      "source": [
        "## Final training script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_m5i0p2hCL6"
      },
      "source": [
        "from transformers import TrainingArguments \n",
        "\n",
        "\n",
        "kwargs = TrainingArguments(\n",
        "    fp16=True, per_device_train_batch_size=4, gradient_accumulation_steps=32,\n",
        "    num_train_epochs=30, seed=42, output_dir=output_dir, \n",
        "    evaluation_strategy=\"steps\", per_device_eval_batch_size=4, \n",
        "    eval_accumulation_steps=None, max_grad_norm=None,\n",
        "    lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0,\n",
        "    logging_first_step=True, logging_steps=2000, save_steps=5000, \n",
        "    dataloader_drop_last=True, eval_steps=2000, dataloader_num_workers=2, \n",
        "    deepspeed=None, adafactor=True\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"MyNewT5Tokenizer\")\n",
        "ds_train = My_Dataset(\"my_pandas_train_dataframe_pickle.gz\")\n",
        "ds_eval = My_Dataset(\"my_pandas_valid_dataframe_pickle.gz\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "trainer = MyTrainer(\n",
        "    model=model, data_collator=MyCollator2(tokenizer), tokenizer=tokenizer,\n",
        "    args=kwargs, train_dataset=ds_train, eval_dataset=ds_eval,\n",
        "    optimizers=(MyAdafactor((param for param in model.parameters() if param.requires_grad),\n",
        "                            relative_step=True, warmup_init=True), Dummy()))\n",
        "\n",
        "trainer.remove_callback(TensorBoardCallback)\n",
        "trainer.train(resume_from_checkpoint=None)\n",
        "trainer.save_model(\"output_directory_for_my_model\")\n",
        "trainer.state.save_to_json(\"output_directory_for_my_model/trainer_state.json\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}